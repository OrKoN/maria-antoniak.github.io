---
layout: default
---

# Machine Learning with Words: A Python Primer

If you're curious about machine learning and want to get started coding as quickly as possible, this primer is for you! My goal is to get you running a machine learning pipeline on your own dataset.

You don’t need any background in machine learning to follow this primer, but I’ll assume that you know enough programming to follow my Python snippets.

<br><br>

## Requirements

Everything you need is contained in [Anaconda](https://www.continuum.io/anaconda-overview).

If you prefer to download things manually, you'll need Python 2.7 and the following Python packages:

* [pandas](http://pandas.pydata.org/)
* [scikit-learn](http://scikit-learn.org/)
* [gensim](https://radimrehurek.com/gensim/)
* [spaCy](https://spacy.io/)
* [matplotlib](https://matplotlib.org/)
* [seaborn](https://seaborn.pydata.org/)

You can install all of those packages using [pip](https://packaging.python.org/tutorials/installing-packages/).

I also recommend installing [PyCharm](https://www.jetbrains.com/pycharm/) to make debugging a bit easier. You can use a [Jupyter notebook](http://jupyter.org/) (included with Anaconda) to write and run your code interactively.

<br><br>

## Find a dataset

You can use any text dataset you like. I’ll be using a collection of Shakespeare plays from [Project Gutenberg](https://www.gutenberg.org/).

To make your first project easier, you might want to choose a dataset with the following qualities:

* machine-readable text (.txt or .csv files, not PDF)
* 1,000-100,000 documents
* a topic that is familiar to you

<br><br>

## Load and clean your dataset

A **document** is a unit within your dataset. How big should it be? That's up to you! A document might be one sentence, one paragraph, one chapter, one tweet, one book...

Use **pandas** to load your data and divide it into documents.

{% highlight python %}
import pandas as pd

pd.load_csv('/path/to/your/documents')
{% endhighlight %}

Clean the documents by removing numbers and punctuation, lowercasing the text, and removing words that occur fewer than N times.

{% highlight python %}
def remove_non_alpha(text):
    return re.sub('[^A-Za-z\s]', ' ', text)

def preprocess_text(text, stopwords=None):
    text = text.lower()
    text = remove_non_alpha(text)
    return text
{% endhighlight %}

<br><br>

## Create feature vectors.

A [feature vector](https://en.wikipedia.org/wiki/Feature_vector) is a machine readable representation of a document. It includes a list of **features** and **values**. The values can either indicate whether a feature is present or how important it is. For text documents, feature vectors usually consist of words (features) and their frequencies (values).

For example, the sentence *A fool thinks himself to be wise, but a wise man knows himself to be a fool.* can be transformed into the following feature vector:

| a  | be  | but  | fool  | himself | knows | man  | thinks  | to  | wise  |   
|----|-----|------|-------|---------|-------|------|---------|-----|-------|
| 3  | 2   | 1    | 2     | 2       | 1     | 1    | 1       | 2   | 2     |

Unfortunately, using just the words and frequencies can result in some errors. Here are a few problems to consider:

* Documents have different lengths, so the frequency of a word within one document might not be comparable to its frequency in another document.

* Some words occur very frequently and are distributed (somewhat) evenly across all documents (e.g. *the*, *and*, *she*). We sometimes call these [stopwords](https://en.wikipedia.org/wiki/Stop_words).

* Many, many words occur only a few times in the entire dataset. This is known as the [long tail](https://en.wikipedia.org/wiki/Long_tail).

To solve these problems, we'll ignore rare words when constructing our features, and we'll weight our feature values using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency-inverse document frequency).

Luckily, this is all very easy if we use **scikit-learn** to create our feature vectors.

{% highlight python %}
vectorizer = TfidfVectorizer(norm=l2, use_idf=True, min_df=10, sublinear_tf=True)
vectorizer = vectorizer.fit(documents)
vectorizer.transform(documents)
{% endhighlight %}

By specifying `min_df=10` we've required that each word occur in at least 10 documents. This eliminates rare words from our features. By specifying, `norm=l2`, we normalize the feature values to remove the bias introduced by different document lengths. By specifying `sublinear_tf=True`, we scale the feature values so that very frequent features don't overwhelm the vectors.

Further Reading:
* https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html
* http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
* http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/

<br><br>

## Cluster your dataset

Ok, time for some magic!

Let's ask the computer to _cluster_ your dataset. Clustering means gathering the documents into groups based on their similarity.

How it works: K-means searches for

<br><br>

## Discover topics in your dataset


## Label and classify your dataset


## Resources to learn more


<br>

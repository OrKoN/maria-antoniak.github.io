---
layout: default
---

# Machine Learning with Words: A Python Primer

This is a quick start primer to machine learning techniques for text data. You don’t need any background in machine learning to follow this primer, but I’ll assume that you know enough programming to understand my Python snippets.

My goal is to get you running a machine learning pipeline on your own dataset as quickly as possible. I want you to experience the magic of machine learning and start building cool things.

## Requirements

You will need Python 2.7 and the following Python packages:

* pandas
* scikit-learn
* gensim
* spaCy
* matplotlib
* seaborn

If you are new to Python, I recommend installing [Anaconda](https://www.continuum.io/anaconda-overview), which includes all of the above packages.

I also recommend installing [PyCharm](https://www.jetbrains.com/pycharm/) to make debugging a bit easier. You can use a [Jupyter notebook](http://jupyter.org/) (included with Anaconda) to write and run your code interactively.


## Find a dataset

You can use any text dataset you like. I’ll be using a collection of Shakespeare plays from [Project Gutenberg](https://www.gutenberg.org/).

To make your first project easier, you might want to choose a dataset with the following qualities:

* machine-readable text (.txt or .csv files, not PDF)
* 1,000-100,000 documents
* a topic that is familiar to you


## Load and clean your dataset

A **document** is a unit within your dataset. How big should it be? That's up to you! A document might be one sentence, one paragraph, one chapter, one tweet, one book... My document size will be contiguous lines from a single character.

Use _pandas_ to load your data and divide it into documents.

```python
import pandas as pd

pd.load_csv('/path/to/your/documents')
```

Clean the documents by removing numbers and punctuation, lowercasing the text, and removing words that occur fewer than N times.

```python
def remove_non_alpha(text):
    return re.sub('[^A-Za-z\s]', ' ', text)

def preprocess_text(text, stopwords=None):
    text = text.lower()
    text = remove_non_alpha(text)
    return text
```

## Create feature vectors.

A [feature vector](https://en.wikipedia.org/wiki/Feature_vector) is a machine readable representation of a single document. It includes a list of **features** (words) and **values** (word frequencies). You could add extra features, such as the year or author, but for now, we’ll just use words and their frequencies.

We need to transform all of the documents into feature vectors so that the machine learning algorithm knows which aspects of the document are important for learning.

When working with text, there are some important things to consider when creating your feature vectors:

### Document Lengths
Some of your documents are different lengths, so the frequency of a word within the document isn't comparable to its frequency in other documents.

### Frequent Words
A few words occur very frequently across all documents (e.g. *the*, *and*, *she*) but aren't significant. We call these [stopwords](https://en.wikipedia.org/wiki/Stop_words).

### Rare Words
Many words only occur a few times and aren't significant. This is known as the [long tail](https://en.wikipedia.org/wiki/Long_tail).

To solve these problems, we won't use the simple word frequency. Instead, we'll store a weighted frequency called **TF-IDF** (term frequency-inverse document frequency), and we'll ignore rare words when constructing our features.

Luckily, this is all very easy if we use **scikit-learn** to create our feature vectors.

```python
vectorizer = TfidfVectorizer(use_idf=True, min_df=10, sublinear_tf=True)
vectorizer = vectorizer.fit(documents)
vectorizer.transform(documents)
```

By specifying `min_df=10` we've required that each word occur in at least 10 documents. This eliminates infrequent words from our features.

How it works: TF-IDF weights the word frequencies


## Cluster your dataset

Ok, time for some magic!

Let's ask the computer to _cluster_ your dataset. Clustering means gathering the documents into groups based on their similarity.

How it works: K-means searches for


## Discover topics in your dataset


## Label and classify your dataset


## Resources to learn more


<br>
